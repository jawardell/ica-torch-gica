{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_id:001312269620\n",
      "func_file:/data/qneuromark/Data/FBIRN/ZN_Neuromark/ZN_Prep_fMRI/001312269620/SM.nii\n",
      "output_dir:./out/\n",
      "mask_file:/data/users2/jwardell1/nshor_docker/examples/fbirn-project/FBIRN/group_mean_masks/mask_resampled.nii\n",
      "template_file:/data/users2/jwardell1/ica-torch-gica/sa_script_work/gica/group_level_analysis/Neuromark_fMRI_1.0.nii\n",
      "src_data.shape torch.Size([157, 58104])\n",
      "ref_data.shape torch.Size([53, 58104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3944648/3958881146.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  FmriMatr = torch.tensor(FmriMatr, dtype=torch.float64)\n",
      "/tmp/ipykernel_3944648/3958881146.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ICRefMax = torch.tensor(ICRefMax, dtype=torch.float64)\n",
      "2024-03-05 17:09:21,690 - pygigicar - INFO - Starting with EGv=0.374567\n",
      "2024-03-05 17:09:21,694 - pygigicar - INFO - gigicar component: 0/53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc\n",
      "tensor(0.0029)\n",
      "init sources\n",
      "tensor(9.0831e-15)\n",
      "tensor(166.3231)\n",
      "loss  0\n",
      "tensor(-0.7462, grad_fn=<SumBackward0>)\n",
      "loss  1\n",
      "tensor(-0.7463, grad_fn=<SumBackward0>)\n",
      "loss  2\n",
      "tensor(-0.7465, grad_fn=<SumBackward0>)\n",
      "loss  3\n",
      "tensor(-0.7466, grad_fn=<SumBackward0>)\n",
      "loss  4\n",
      "tensor(-0.7468, grad_fn=<SumBackward0>)\n",
      "loss  5\n",
      "tensor(-0.7469, grad_fn=<SumBackward0>)\n",
      "loss  6\n",
      "tensor(-0.7470, grad_fn=<SumBackward0>)\n",
      "loss  7\n",
      "tensor(-0.7472, grad_fn=<SumBackward0>)\n",
      "loss  8\n",
      "tensor(-0.7473, grad_fn=<SumBackward0>)\n",
      "loss  9\n",
      "tensor(-0.7475, grad_fn=<SumBackward0>)\n",
      "loss  10\n",
      "tensor(-0.7476, grad_fn=<SumBackward0>)\n",
      "loss  11\n",
      "tensor(-0.7477, grad_fn=<SumBackward0>)\n",
      "loss  12\n",
      "tensor(-0.7479, grad_fn=<SumBackward0>)\n",
      "loss  13\n",
      "tensor(-0.7480, grad_fn=<SumBackward0>)\n",
      "loss  14\n",
      "tensor(-0.7481, grad_fn=<SumBackward0>)\n",
      "loss  15\n",
      "tensor(-0.7483, grad_fn=<SumBackward0>)\n",
      "loss  16\n",
      "tensor(-0.7484, grad_fn=<SumBackward0>)\n",
      "loss  17\n",
      "tensor(-0.7485, grad_fn=<SumBackward0>)\n",
      "loss  18\n",
      "tensor(-0.7486, grad_fn=<SumBackward0>)\n",
      "loss  19\n",
      "tensor(-0.7488, grad_fn=<SumBackward0>)\n",
      "loss  20\n",
      "tensor(-0.7489, grad_fn=<SumBackward0>)\n",
      "loss  21\n",
      "tensor(-0.7490, grad_fn=<SumBackward0>)\n",
      "loss  22\n",
      "tensor(-0.7491, grad_fn=<SumBackward0>)\n",
      "loss  23\n",
      "tensor(-0.7492, grad_fn=<SumBackward0>)\n",
      "loss  24\n",
      "tensor(-0.7494, grad_fn=<SumBackward0>)\n",
      "loss  25\n",
      "tensor(-0.7495, grad_fn=<SumBackward0>)\n",
      "loss  26\n",
      "tensor(-0.7496, grad_fn=<SumBackward0>)\n",
      "loss  27\n",
      "tensor(-0.7497, grad_fn=<SumBackward0>)\n",
      "loss  28\n",
      "tensor(-0.7498, grad_fn=<SumBackward0>)\n",
      "loss  29\n",
      "tensor(-0.7499, grad_fn=<SumBackward0>)\n",
      "loss  30\n",
      "tensor(-0.7500, grad_fn=<SumBackward0>)\n",
      "loss  31\n",
      "tensor(-0.7501, grad_fn=<SumBackward0>)\n",
      "loss  32\n",
      "tensor(-0.7502, grad_fn=<SumBackward0>)\n",
      "loss  33\n",
      "tensor(-0.7503, grad_fn=<SumBackward0>)\n",
      "loss  34\n",
      "tensor(-0.7504, grad_fn=<SumBackward0>)\n",
      "loss  35\n",
      "tensor(-0.7505, grad_fn=<SumBackward0>)\n",
      "loss  36\n",
      "tensor(-0.7506, grad_fn=<SumBackward0>)\n",
      "loss  37\n",
      "tensor(-0.7507, grad_fn=<SumBackward0>)\n",
      "loss  38\n",
      "tensor(-0.7508, grad_fn=<SumBackward0>)\n",
      "loss  39\n",
      "tensor(-0.7509, grad_fn=<SumBackward0>)\n",
      "loss  40\n",
      "tensor(-0.7510, grad_fn=<SumBackward0>)\n",
      "loss  41\n",
      "tensor(-0.7511, grad_fn=<SumBackward0>)\n",
      "loss  42\n",
      "tensor(-0.7512, grad_fn=<SumBackward0>)\n",
      "loss  43\n",
      "tensor(-0.7513, grad_fn=<SumBackward0>)\n",
      "loss  44\n",
      "tensor(-0.7514, grad_fn=<SumBackward0>)\n",
      "loss  45\n",
      "tensor(-0.7515, grad_fn=<SumBackward0>)\n",
      "loss  46\n",
      "tensor(-0.7516, grad_fn=<SumBackward0>)\n",
      "loss  47\n",
      "tensor(-0.7517, grad_fn=<SumBackward0>)\n",
      "loss  48\n",
      "tensor(-0.7518, grad_fn=<SumBackward0>)\n",
      "loss  49\n",
      "tensor(-0.7519, grad_fn=<SumBackward0>)\n",
      "loss  50\n",
      "tensor(-0.7519, grad_fn=<SumBackward0>)\n",
      "loss  51\n",
      "tensor(-0.7520, grad_fn=<SumBackward0>)\n",
      "loss  52\n",
      "tensor(-0.7521, grad_fn=<SumBackward0>)\n",
      "loss  53\n",
      "tensor(-0.7522, grad_fn=<SumBackward0>)\n",
      "loss  54\n",
      "tensor(-0.7523, grad_fn=<SumBackward0>)\n",
      "loss  55\n",
      "tensor(-0.7524, grad_fn=<SumBackward0>)\n",
      "loss  56\n",
      "tensor(-0.7524, grad_fn=<SumBackward0>)\n",
      "loss  57\n",
      "tensor(-0.7525, grad_fn=<SumBackward0>)\n",
      "loss  58\n",
      "tensor(-0.7526, grad_fn=<SumBackward0>)\n",
      "loss  59\n",
      "tensor(-0.7527, grad_fn=<SumBackward0>)\n",
      "loss  60\n",
      "tensor(-0.7528, grad_fn=<SumBackward0>)\n",
      "loss  61\n",
      "tensor(-0.7528, grad_fn=<SumBackward0>)\n",
      "loss  62\n",
      "tensor(-0.7529, grad_fn=<SumBackward0>)\n",
      "loss  63\n",
      "tensor(-0.7530, grad_fn=<SumBackward0>)\n",
      "loss  64\n",
      "tensor(-0.7531, grad_fn=<SumBackward0>)\n",
      "loss  65\n",
      "tensor(-0.7531, grad_fn=<SumBackward0>)\n",
      "loss  66\n",
      "tensor(-0.7532, grad_fn=<SumBackward0>)\n",
      "loss  67\n",
      "tensor(-0.7533, grad_fn=<SumBackward0>)\n",
      "loss  68\n",
      "tensor(-0.7534, grad_fn=<SumBackward0>)\n",
      "loss  69\n",
      "tensor(-0.7534, grad_fn=<SumBackward0>)\n",
      "loss  70\n",
      "tensor(-0.7535, grad_fn=<SumBackward0>)\n",
      "loss  71\n",
      "tensor(-0.7536, grad_fn=<SumBackward0>)\n",
      "loss  72\n",
      "tensor(-0.7536, grad_fn=<SumBackward0>)\n",
      "loss  73\n",
      "tensor(-0.7537, grad_fn=<SumBackward0>)\n",
      "loss  74\n",
      "tensor(-0.7538, grad_fn=<SumBackward0>)\n",
      "loss  75\n",
      "tensor(-0.7538, grad_fn=<SumBackward0>)\n",
      "loss  76\n",
      "tensor(-0.7539, grad_fn=<SumBackward0>)\n",
      "loss  77\n",
      "tensor(-0.7540, grad_fn=<SumBackward0>)\n",
      "loss  78\n",
      "tensor(-0.7540, grad_fn=<SumBackward0>)\n",
      "loss  79\n",
      "tensor(-0.7541, grad_fn=<SumBackward0>)\n",
      "loss  80\n",
      "tensor(-0.7542, grad_fn=<SumBackward0>)\n",
      "loss  81\n",
      "tensor(-0.7542, grad_fn=<SumBackward0>)\n",
      "loss  82\n",
      "tensor(-0.7543, grad_fn=<SumBackward0>)\n",
      "loss  83\n",
      "tensor(-0.7544, grad_fn=<SumBackward0>)\n",
      "loss  84\n",
      "tensor(-0.7544, grad_fn=<SumBackward0>)\n",
      "loss  85\n",
      "tensor(-0.7545, grad_fn=<SumBackward0>)\n",
      "loss  86\n",
      "tensor(-0.7545, grad_fn=<SumBackward0>)\n",
      "loss  87\n",
      "tensor(-0.7546, grad_fn=<SumBackward0>)\n",
      "loss  88\n",
      "tensor(-0.7547, grad_fn=<SumBackward0>)\n",
      "loss  89\n",
      "tensor(-0.7547, grad_fn=<SumBackward0>)\n",
      "loss  90\n",
      "tensor(-0.7548, grad_fn=<SumBackward0>)\n",
      "loss  91\n",
      "tensor(-0.7548, grad_fn=<SumBackward0>)\n",
      "loss  92\n",
      "tensor(-0.7549, grad_fn=<SumBackward0>)\n",
      "loss  93\n",
      "tensor(-0.7549, grad_fn=<SumBackward0>)\n",
      "loss  94\n",
      "tensor(-0.7550, grad_fn=<SumBackward0>)\n",
      "loss  95\n",
      "tensor(-0.7550, grad_fn=<SumBackward0>)\n",
      "loss  96\n",
      "tensor(-0.7551, grad_fn=<SumBackward0>)\n",
      "loss  97\n",
      "tensor(-0.7552, grad_fn=<SumBackward0>)\n",
      "loss  98\n",
      "tensor(-0.7552, grad_fn=<SumBackward0>)\n",
      "loss  99\n",
      "tensor(-0.7553, grad_fn=<SumBackward0>)\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.linalg import norm\n",
    "import nibabel as nib\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "a = 0.5\n",
    "b = 1 - a\n",
    "EGv = 0.3745672075\n",
    "ErChuPai = 2 / 3.141592653589793  \n",
    "\n",
    "\n",
    "\n",
    "def joint_loss(sources, mag_norm, reference, m):\n",
    "    #m = number of time steps\n",
    "    #reference = group map\n",
    "    #y3 = output\n",
    "    #c = magnitude normalization\n",
    "    #Cosy1 = torch.cosh(sources)\n",
    "    #logCosy1 = torch.log(Cosy1)\n",
    "    #EGy1 = logCosy1.mean()\n",
    "    #Negama = EGy1 - EGv\n",
    "    #Jy1 = (EGy1 - EGv)**2\n",
    "    #KwDaoshu = ErChuPai * mag_norm * (1 / (1 + (mag_norm * Jy1)**2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #normalize weights for testing, normalize sources: check diff of sources@ref.t\n",
    "    loss = -(a * ErChuPai * torch.arctan(mag_norm * nege(sources)) + b * (1 / m) * sources.t() @ reference)\n",
    "    #loss = (a * KwDaoshu * 2 * Negama * EYgy + b * Simgrad.squeeze())\n",
    "    return loss\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class gigICA(torch.nn.Module):\n",
    "    #init_sources = y1, init_weights = 2c, mag_norm = c, m = m\n",
    "    def __init__(self, init_sources, init_weights, dim, mag_norm, m):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(init_weights.shape[0], 1, bias=False)\n",
    "        self.W.weight = nn.parameter.Parameter(init_weights.reshape([1,-1]))\n",
    "        self.mag_norm = mag_norm \n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, X, reference, last_sources):\n",
    "        sources = self.W(X)\n",
    "        #Simgrad = (1 / self.m) * X @ reference.t()\n",
    "        Eyr = 1/self.m * (sources.squeeze() @ reference.squeeze())\n",
    "        \n",
    "        \n",
    "        Simgrad = (1 / self.m) * X.t() @ reference\n",
    "        \n",
    "        return sources\n",
    "        \n",
    "        \n",
    "'''\n",
    "iternum = 10\n",
    "    \n",
    "#wc = wc / norm(wc)\n",
    "#y1 = wc.t() @ Y     \n",
    "Cosy1 = torch.cosh(y1)\n",
    "logCosy1 = torch.log(Cosy1)\n",
    "EGy1 = logCosy1.mean()\n",
    "Negama = EGy1 - EGv\n",
    "#as input\n",
    "#dim = y1.shape[0] if len(y1.shape) > 1 else y1.shape[0]\n",
    "EYgy = (1 / m) * Y @ (torch.tanh(y1)).t()\n",
    "Jy1 = (EGy1 - EGv)**2\n",
    "KwDaoshu = ErChuPai * c * (1 / (1 + (c * Jy1)**2))\n",
    "Simgrad = (1 / m) * Y @ reference.t()\n",
    "\n",
    "\n",
    "Gradient calculation:\n",
    "\n",
    "g = a * KwDaoshu * 2 * Negama * EYgy + b * Simgrad#.view(Simgrad.shape[0], 1)\n",
    "g_norm = torch.sqrt(g.T@g)#torch.linalg.norm(g)\n",
    "d = g / g_norm\n",
    "wx = wc + Nemda * d #wc.view(wc.shape[0], 1)\n",
    "wx = wx / norm(wx)\n",
    "y3 = wx.t() @ Y\n",
    "\n",
    "\n",
    "Loss computation:\n",
    "#Loss term\n",
    "PreObjValue = a * ErChuPai * torch.arctan(c * nege(y3)) + b * (1 / m) * y3 @ reference.t()\n",
    "ObjValueChange = PreObjValue - IniObjValue\n",
    "ftol = 0.02\n",
    "dg = g.t() @ d\n",
    "#In body of loop\n",
    "ArmiCondiThr = Nemda * ftol * dg\n",
    "if ObjValueChange < ArmiCondiThr:\n",
    "    Nemda = Nemda / 2\n",
    "    continue\n",
    "if torch.allclose(wc.t() @ wx, torch.zeros(1), atol=1e-5):\n",
    "    break\n",
    "elif itertime == iternum:\n",
    "    break\n",
    "IniObjValue = PreObjValue\n",
    "y1 = y3\n",
    "wc = wx\n",
    "itertime = itertime + 1\n",
    "'''      \n",
    "        \n",
    "def gigicar(FmriMatr, ICRefMax):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    FmriMatr = torch.tensor(FmriMatr, dtype=torch.float64)\n",
    "    ICRefMax = torch.tensor(ICRefMax, dtype=torch.float64)\n",
    "\n",
    "    # Extract dimensions\n",
    "    n, m = FmriMatr.shape\n",
    "    n2, m2 = ICRefMax.shape\n",
    "\n",
    "    # Subtract mean from observed data\n",
    "    #FmriMat = FmriMatr - FmriMatr.mean(dim=1, keepdim=True)\n",
    "    FmriMat=FmriMatr - torch.tile(torch.mean(FmriMatr,1),(m,1)).T\n",
    "    # Calculate covariance matrix\n",
    "    CovFmri = (FmriMat @ FmriMat.t()) / m\n",
    "\n",
    "    # Perform PCA reduction on signal\n",
    "    D, E = torch.linalg.eig(CovFmri)\n",
    "\n",
    "    #D = D[:, 0].real if len(D.shape) > 1 else D.real  # Extract real parts of eigenvalues\n",
    "    EsICnum = ICRefMax.shape[0]\n",
    "    D = D.real\n",
    "    # Sort eigenvalues and eigenvectors\n",
    "    index = D.argsort()\n",
    "    eigenvalues = D[index]\n",
    "    cols=E.shape[1]\n",
    "    Esort=torch.zeros(E.shape)\n",
    "    dsort=torch.zeros(eigenvalues.shape)\n",
    "    for i in range(cols):\n",
    "        Esort[:,i] = E[:,index[cols-i-1] ]\n",
    "        dsort[i]   = D[index[cols-i-1] ]\n",
    "\n",
    "\n",
    "    thr = 0  # Set your threshold value here\n",
    "    numpc = (dsort > thr).sum()\n",
    "\n",
    "    # Perform PCA for selected components\n",
    "    Epart = Esort[:, :numpc]#.real\n",
    "    dpart = dsort[:numpc]\n",
    "    Lambda_part = torch.diag(dpart)#.real\n",
    "    # Whitening source signal\n",
    "    tmp = torch.sqrt(Lambda_part)\n",
    "    Lambda_inv = torch.linalg.inv(torch.sqrt(Lambda_part)) \n",
    "    WhitenMatrix = Lambda_inv @ Epart.t()\n",
    "    Y = WhitenMatrix @ FmriMat\n",
    "    if thr<1e-10 and numpc<n:\n",
    "        for i in range(Y.shape[0]):\n",
    "            Y[i,:]=Y[i,:]/torch.std(Y[i,:])\n",
    "    # Normalize source signal\n",
    "    #Y = F.normalize(Y, dim=1)\n",
    "    # Normalize reference signals\n",
    "    ICRefMaxC=ICRefMax - torch.tile(torch.mean(ICRefMax,1), (m2, 1)).T\n",
    "    ICRefMaxN=torch.zeros((EsICnum,m2))\n",
    "    for i in range(EsICnum):\n",
    "        ICRefMaxN[i,:]=ICRefMaxC[i,:]/torch.std(ICRefMaxC[i,:])\n",
    "    #ICRefMaxN = (ICRefMax - ICRefMax.mean(dim=1, keepdim=True)) / ICRefMax.std(dim=1, keepdim=True)\n",
    "\n",
    "    # Computing negentropy\n",
    "    NegeEva = torch.zeros((EsICnum, 1))\n",
    "    for i in range(EsICnum):\n",
    "        NegeEva[i] = nege(ICRefMaxN[i, :])\n",
    "\n",
    "    iternum = 100\n",
    "    a = 0.5\n",
    "    b = 1 - a\n",
    "    EGv = 0.3745672075\n",
    "    ErChuPai = 2 / 3.141592653589793\n",
    "\n",
    "    ICOutMax = torch.zeros((EsICnum, m))\n",
    "    logger.info(\"Starting with EGv=%f\" % EGv)\n",
    "    \n",
    "    gradients = []\n",
    "    for ICnum in range(1):\n",
    "        logger.info('gigicar component: %d/%d' % (ICnum, EsICnum))\n",
    "        reference = ICRefMaxN[ICnum, :]\n",
    "        wc = (reference @ torch.linalg.pinv(Y)).t()\n",
    "        wc = wc / norm(wc)\n",
    "        print(\"wc\")\n",
    "        print(torch.mean(wc))\n",
    "        last_sources = wc.t() @ Y\n",
    "        EyrInitial = (1 / m) * (last_sources) @ reference.t()\n",
    "        NegeInitial = nege(last_sources)\n",
    "        mag_norm = (torch.tan((EyrInitial * 3.141592653589793) / 2)) / NegeInitial\n",
    "        IniObjValue = a * ErChuPai * torch.arctan(mag_norm * NegeInitial) + b * EyrInitial\n",
    "\n",
    "        itertime = 1\n",
    "        Nemda = 1\n",
    "        print(\"init sources\")\n",
    "        print(torch.mean(last_sources))\n",
    "        GICA = gigICA(last_sources,wc,0,mag_norm,m)\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.SGD(GICA.parameters(), lr=.001)\n",
    "        grrs = []\n",
    "        print(mag_norm)\n",
    "        for i in range(iternum):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            sources = GICA(Y.t(),reference.reshape([-1,1]), last_sources.reshape([-1,1]))\n",
    "            loss = joint_loss(sources, mag_norm, reference.reshape([-1,1]), m).sum()\n",
    "            loss.backward()\n",
    "            print(\"loss \",i)\n",
    "            print(loss)\n",
    "            gtmp = GICA.W.weight.grad.cpu().numpy()[0]\n",
    "            grrs.append(gtmp)\n",
    "            optimizer.step()\n",
    "            #Weight norm here...\n",
    "            #GICA.W.weight = torch.nn.Parameter(GICA.W.weight/norm(GICA.W.weight))\n",
    "            itertime = itertime + 1\n",
    "        wx = GICA.W.state_dict()['weight']\n",
    "        Source = wx @ Y\n",
    "        gradients.append(np.array(grrs))\n",
    "        ICOutMax[ICnum, :] = Source.squeeze()\n",
    "    np.save(f'{output_dir}/{sub_id}_grads.npy',np.array(gradients))\n",
    "    TCMax = (1 / m) * FmriMatr @ ICOutMax.t()\n",
    "    return ICOutMax, TCMax\n",
    "\n",
    "\n",
    "def nege(x):\n",
    "    y = torch.log(torch.cosh(x))\n",
    "    E1 = y.mean()\n",
    "    E2 = 0.3745672075\n",
    "    return (E1 - E2)**2\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# SETUP LOGGER\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_REFERENCE_FN = 'pooled_47.nii'\n",
    "DEFAULT_EXAMPLE_FN = 'example.nii'\n",
    "FORMAT = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(filename='pygigicar.log',level=logging.INFO)\n",
    "logging.basicConfig(format=FORMAT)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "\n",
    "logger = logging.getLogger('pygigicar')\n",
    "logger.setLevel(logging.INFO)\n",
    "# add ch to logger\n",
    "logger.handlers = []\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# CALL FUNCTIONS\n",
    "########################################################################\n",
    "'''\n",
    "if len(sys.argv) != 6:\n",
    "    print(\"Usage: python gigicar.py sub_id func_file out_dir mask_file template_file \")\n",
    "    print(sys.argv)\n",
    "    sys.exit()\n",
    "'''\n",
    "#sub_id = sys.argv[1]\n",
    "sub_id = '000300655084'\n",
    "sub_id = \"001312269620\"\n",
    "#sub_id = 'test'\n",
    "print(f\"sub_id:{sub_id}\")\n",
    "\n",
    "#func_file = sys.argv[2]\n",
    "func_file = '/data/qneuromark/Data/FBIRN/ZN_Neuromark/ZN_Prep_fMRI/'+sub_id+'/SM.nii'\n",
    "print(f\"func_file:{func_file}\")\n",
    "\n",
    "#output_dir = sys.argv[3]\n",
    "output_dir = './out/'\n",
    "print(f\"output_dir:{output_dir}\")\n",
    "\n",
    "#mask_file = sys.argv[4]\n",
    "mask_file = '/data/users2/jwardell1/nshor_docker/examples/fbirn-project/FBIRN/group_mean_masks/mask_resampled.nii'\n",
    "print(f\"mask_file:{mask_file}\")\n",
    "\n",
    "#template_file = sys.argv[5]\n",
    "template_file = '/data/users2/jwardell1/ica-torch-gica/sa_script_work/gica/group_level_analysis/Neuromark_fMRI_1.0.nii'\n",
    "print(f\"template_file:{template_file}\")\n",
    "\n",
    "'''\n",
    "if not os.path.isfile(func_file):\n",
    "    print(\"Error: subject's preprocessed fMRI file not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    print(\"Error: output dir not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if not os.path.isfile(mask_file):\n",
    "    print(\"Error: mask file not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if not os.path.isfile(template_file):\n",
    "    print(\"Error: template file not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load images\n",
    "src_img = nib.load(func_file)\n",
    "src_data = torch.tensor(src_img.get_fdata(), dtype=torch.float64)\n",
    "\n",
    "ref_img = nib.load(template_file)\n",
    "ref_data = torch.tensor(ref_img.get_fdata(), dtype=torch.float64)\n",
    "\n",
    "mask_img = nib.load(mask_file)\n",
    "mask_data = torch.tensor(mask_img.get_fdata(), dtype=torch.float64)\n",
    "\n",
    "# Create idx tensor\n",
    "idx = torch.nonzero(mask_data).t()\n",
    "\n",
    "# Mask source and reference images\n",
    "src_data = src_data[idx[0], idx[1], idx[2], :].t()\n",
    "print(f'src_data.shape {src_data.shape}')\n",
    "\n",
    "ref_data = ref_data[idx[0], idx[1], idx[2], :].t()\n",
    "print(f'ref_data.shape {ref_data.shape}')\n",
    "\n",
    "# Convert idx to numpy for compatibility with existing code\n",
    "idx_np = idx.cpu().numpy()\n",
    "\n",
    "# Continue with the rest of your PyTorch code...\n",
    "ICOutMax, TCMax = gigicar(src_data, ref_data)\n",
    "\n",
    "\n",
    "TCMax = TCMax.cpu().numpy()\n",
    "# Save time courses file\n",
    "tcfilename = f'{output_dir}/{sub_id}_TCMax_Torchified.npy'\n",
    "np.save(tcfilename, TCMax)\n",
    "\n",
    "# Reconstruct brain voxels\n",
    "xdim, ydim, zdim = mask_data.shape\n",
    "n_comp = ICOutMax.shape[0]\n",
    "image_stack = torch.zeros((xdim, ydim, zdim, n_comp))\n",
    "\n",
    "# Convert idx to numpy for compatibility with existing code\n",
    "idx_np = idx.cpu().numpy()\n",
    "\n",
    "image_stack[idx_np[0], idx_np[1], idx_np[2], :] = ICOutMax.t()\n",
    "\n",
    "# Save as nifti\n",
    "print(\"saving\")\n",
    "nifti_img = nib.Nifti1Image(image_stack.numpy(), affine=mask_img.get_qform())\n",
    "nifti_img.header.set_sform(mask_img.header.get_sform(), code=mask_img.get_qform('code')[1])\n",
    "nifti_img.header.set_qform(mask_img.header.get_qform(), code=mask_img.get_qform('code')[1])\n",
    "nifti_file = f'{output_dir}/{sub_id}_ICOutMax_Torchified_SQ.nii.gz'\n",
    "nib.save(nifti_img, nifti_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaUpdatedPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
